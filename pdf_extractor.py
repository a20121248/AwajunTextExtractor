import os
import re
from pathlib import Path
from typing import List, Optional
from docling.document_converter import DocumentConverter
from langdetect import detect, DetectorFactory, LangDetectException

# Para resultados consistentes en langdetect
DetectorFactory.seed = 0


class PDFExtractor:
    """Clase para extraer texto de PDFs usando docling."""

    def __init__(self):
        self.converter = DocumentConverter()

    def extraer_texto_pdf(self, ruta_pdf: str) -> str:
        """Extrae texto de un PDF usando docling."""
        try:
            result = self.converter.convert(ruta_pdf)
            doc = result.document
            return doc.export_to_markdown()
        except Exception as e:
            print(f"‚ùå Error al procesar {ruta_pdf}: {e}")
            return ""

    def procesar_pdfs(self, carpeta_origen: str, carpeta_destino: Optional[str] = None,
                      formato_salida: str = "md") -> None:
        """Procesa todos los PDFs de una carpeta y guarda el resultado en carpeta destino."""
        carpeta_origen_path = Path(carpeta_origen)

        # Si no se especifica carpeta destino, usar la misma carpeta origen
        if carpeta_destino is None:
            carpeta_destino_path = carpeta_origen_path
        else:
            carpeta_destino_path = Path(carpeta_destino)
            # Crear la carpeta destino si no existe
            carpeta_destino_path.mkdir(parents=True, exist_ok=True)

        if not carpeta_origen_path.exists():
            print(f"‚ùå La carpeta {carpeta_origen} no existe")
            return

        archivos_pdf = list(carpeta_origen_path.glob("*.pdf"))

        if not archivos_pdf:
            print(f"‚ö†Ô∏è No se encontraron archivos PDF en {carpeta_origen}")
            return

        for ruta_pdf in archivos_pdf:
            contenido = self.extraer_texto_pdf(str(ruta_pdf))

            if contenido:
                extension = ".md" if formato_salida == "md" else ".txt"
                nombre_out = ruta_pdf.stem + extension
                # Guardar en la carpeta destino
                ruta_out = carpeta_destino_path / nombre_out

                with open(ruta_out, "w", encoding="utf-8") as f:
                    f.write(contenido)

                print(f"‚úÖ Procesado: {ruta_pdf.name} ‚Üí {carpeta_destino_path / nombre_out}")


class CorpusGenerator:
    """Clase para generar corpus monoling√ºes en Awaj√∫n."""

    def __init__(self):
        # Patrones de ruido t√©cnico para eliminar
        self.patrones_ruido = [
            r'<!-- image -->',  # Marcadores de imagen
            r'^\d+$',  # Solo n√∫meros
            r'^[IVX]+\.$',  # Numeraci√≥n romana
            r'^#{1,6}\s+$',  # Headers vac√≠os
            r'^\s*-+\s*$',  # L√≠neas de guiones
            r'^\s*=+\s*$',  # L√≠neas de igual
            r'^\s*\*+\s*$',  # L√≠neas de asteriscos
        ]

        # Palabras/frases que indican contenido administrativo en espa√±ol
        self.indicadores_administrativo = [
            'evaluaci√≥n', 'resultados', 'estimado', 'padre', 'madre', 'familia',
            'ministerio', 'universidad', 'departamento', 'regi√≥n', 'provincia',
            'nivel', 'grado', 'a√±o', 'fecha', 'p√°gina', '√≠ndice', 'cap√≠tulo',
            'anexo', 'bibliograf√≠a', 'referencias', 'tabla', 'gr√°fico',
            'pregunta', 'respuesta', 'instrucci√≥n', 'procedimiento'
        ]

    def es_ruido_tecnico(self, texto: str) -> bool:
        """Detecta si una l√≠nea es ruido t√©cnico."""
        for patron in self.patrones_ruido:
            if re.match(patron, texto.strip(), re.IGNORECASE):
                return True
        return False

    def es_administrativo(self, texto: str) -> bool:
        """Detecta si contiene t√©rminos administrativos t√≠picos en espa√±ol."""
        texto_lower = texto.lower()
        for indicador in self.indicadores_administrativo:
            if indicador in texto_lower:
                return True
        return False

    def detectar_idioma(self, texto: str) -> str:
        """Detecta el idioma del texto usando langdetect."""
        try:
            # M√≠nimo 10 caracteres para detecci√≥n confiable
            if len(texto.strip()) < 10:
                return "unknown"

            idioma = detect(texto)
            return idioma
        except LangDetectException:
            return "unknown"

    def es_probablemente_espanol(self, texto: str) -> bool:
        """
        Determina si el texto es espa√±ol usando m√∫ltiples estrategias.
        Prioriza conservar awaj√∫n en caso de duda.
        """
        # Filtros r√°pidos primero
        if self.es_ruido_tecnico(texto):
            return True

        if self.es_administrativo(texto):
            return True

        # Detecci√≥n de idioma
        idioma = self.detectar_idioma(texto)

        # Si detecta espa√±ol claramente, eliminar
        if idioma == "es":
            return True

        # Si detecta otros idiomas conocidos (ingl√©s, franc√©s, etc.), tambi√©n eliminar
        if idioma in ["en", "fr", "it", "pt", "de"]:
            return True

        # Si no puede detectar o detecta algo desconocido, CONSERVAR (podr√≠a ser awaj√∫n)
        return False

    def limpiar_texto(self, texto: str) -> str:
        """Limpia el texto eliminando formato markdown y normalizando."""
        # Remover patrones de markdown y formato
        texto = re.sub(r'#{1,6}\s+', '', texto)  # Headers
        texto = re.sub(r'\*\*([^*]+)\*\*', r'\1', texto)  # Bold
        texto = re.sub(r'\*([^*]+)\*', r'\1', texto)  # Italic
        texto = re.sub(r'`([^`]+)`', r'\1', texto)  # Code
        texto = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', texto)  # Links
        texto = re.sub(r'<!-- image -->', '', texto)  # Marcadores de imagen

        # Limpiar espacios m√∫ltiples y saltos de l√≠nea
        texto = re.sub(r'\n+', '\n', texto)
        texto = re.sub(r'[ \t]+', ' ', texto)

        return texto.strip()

    def limpiar_oracion(self, oracion: str) -> str:
        """
        Limpia una oraci√≥n individual aplicando reglas espec√≠ficas para awaj√∫n.
        """
        oracion = oracion.strip()

        # A) Eliminar guiones al inicio de l√≠nea (items de lista)
        oracion = re.sub(r'^-\s+', '', oracion)

        # B) Eliminar n√∫meros/referencias al final de oraciones (. 4, : 8, etc.)
        oracion = re.sub(r'[.:]?\s*\d+\s*$', '', oracion)

        # NUEVA REGLA: Eliminar puntos suspensivos y n√∫meros de √≠ndice
        # Ejemplo: "Yusa ................................. 6" -> "Yusa"
        oracion = re.sub(r'\.{3,}.*?\d+\s*$', '', oracion)
        oracion = re.sub(r'\.{3,}', '', oracion)  # Eliminar puntos suspensivos restantes

        # C & D) Procesar contenido de tablas
        if '|' in oracion:
            # Eliminar filas de separaci√≥n de tablas (solo guiones y pipes)
            if re.match(r'^[\s\|\-]+$', oracion):
                return ""

            # Dividir por pipes y procesar cada celda
            celdas = oracion.split('|')
            celdas_limpias = []

            for celda in celdas:
                celda = celda.strip()
                # Saltar celdas vac√≠as o que solo tienen guiones
                if not celda or re.match(r'^[\-\s]+$', celda):
                    continue

                # Saltar celdas que son solo n√∫meros (n√∫meros de p√°gina/√≠ndice)
                if re.match(r'^\d+$', celda):
                    continue

                # Limpiar puntos suspensivos y n√∫meros en la celda
                celda = re.sub(r'\.{3,}.*?\d+\s*$', '', celda)
                celda = re.sub(r'\.{3,}', '', celda)
                celda = celda.strip()

                # Procesar m√∫ltiples oraciones en una celda (separadas por ‚Ä¢)
                if '‚Ä¢' in celda:
                    sub_oraciones = celda.split('‚Ä¢')
                    for sub_oracion in sub_oraciones:
                        sub_oracion = sub_oracion.strip()
                        if sub_oracion and len(sub_oracion) > 3:
                            celdas_limpias.append(sub_oracion)
                else:
                    if len(celda) > 3:
                        celdas_limpias.append(celda)

            # Si encontramos celdas v√°lidas, retornar la primera v√°lida
            # (las otras se procesar√°n por separado)
            if celdas_limpias:
                return celdas_limpias[0]
            else:
                return ""

        # D) Procesar oraciones que empiezan con ‚Ä¢
        if oracion.startswith('‚Ä¢'):
            oracion = oracion[1:].strip()

        # E) Eliminar comas al inicio de oraci√≥n
        oracion = re.sub(r'^,\s+', '', oracion)

        # Limpiar espacios extra
        oracion = re.sub(r'\s+', ' ', oracion).strip()

        return oracion

    def procesar_tablas(self, texto: str) -> List[str]:
        """
        Extrae todas las celdas v√°lidas de tablas en el texto.
        """
        oraciones_tabla = []
        lineas = texto.split('\n')

        for linea in lineas:
            if '|' in linea:
                # Saltar filas de separaci√≥n
                if re.match(r'^[\s\|\-]+$', linea):
                    continue

                celdas = linea.split('|')
                for celda in celdas:
                    celda = celda.strip()
                    if not celda or re.match(r'^[\-\s]+$', celda):
                        continue

                    # Saltar celdas que son solo n√∫meros
                    if re.match(r'^\d+$', celda):
                        continue

                    # Limpiar puntos suspensivos y n√∫meros
                    celda = re.sub(r'\.{3,}.*?\d+\s*$', '', celda)
                    celda = re.sub(r'\.{3,}', '', celda)
                    celda = celda.strip()

                    if not celda or len(celda) <= 3:
                        continue

                    # Procesar m√∫ltiples oraciones en celda (separadas por ‚Ä¢)
                    if '‚Ä¢' in celda:
                        sub_oraciones = celda.split('‚Ä¢')
                        for sub_oracion in sub_oraciones:
                            sub_oracion_limpia = self.limpiar_oracion(sub_oracion)
                            if sub_oracion_limpia and len(sub_oracion_limpia) > 3:
                                oraciones_tabla.append(sub_oracion_limpia)
                    else:
                        celda_limpia = self.limpiar_oracion(celda)
                        if celda_limpia and len(celda_limpia) > 3:
                            oraciones_tabla.append(celda_limpia)

        return oraciones_tabla

    def segmentar_oraciones(self, texto: str) -> List[str]:
        """
        Segmenta el texto en oraciones, adaptado para awaj√∫n.
        M√°s conservador para preservar texto de lengua de bajos recursos.
        """
        oraciones = []

        # Primero extraer oraciones de tablas
        oraciones_tabla = self.procesar_tablas(texto)
        oraciones.extend(oraciones_tabla)

        # Luego procesar l√≠neas normales (excluyendo las que tienen tablas)
        lineas = texto.split('\n')

        for linea in lineas:
            # Saltar l√≠neas que contienen tablas (ya procesadas)
            if '|' in linea:
                continue

            linea = linea.strip()

            # Saltar l√≠neas muy cortas o vac√≠as
            if len(linea) < 3:
                continue

            # Procesar oraciones con ‚Ä¢ (items de lista)
            if '‚Ä¢' in linea:
                items = linea.split('‚Ä¢')
                for item in items:
                    item_limpio = self.limpiar_oracion(item)
                    if item_limpio and len(item_limpio) >= 3:
                        oraciones.append(item_limpio)
                continue

            # Si la l√≠nea no tiene puntuaci√≥n final, probablemente es una oraci√≥n completa
            if not re.search(r'[.!?;:]$', linea):
                linea_limpia = self.limpiar_oracion(linea)
                if linea_limpia and len(linea_limpia) >= 3:
                    oraciones.append(linea_limpia)
            else:
                # Dividir por puntuaci√≥n, pero conservadoramente
                sub_oraciones = re.split(r'[.!?]+', linea)
                for sub_oracion in sub_oraciones:
                    sub_oracion_limpia = self.limpiar_oracion(sub_oracion)
                    if sub_oracion_limpia and len(sub_oracion_limpia) >= 3:
                        oraciones.append(sub_oracion_limpia)

        return oraciones

    def extraer_oraciones_awajun(self, archivo_texto: str) -> List[str]:
        """Extrae oraciones en awaj√∫n de un archivo de texto."""
        try:
            with open(archivo_texto, 'r', encoding='utf-8') as f:
                contenido = f.read()
        except Exception as e:
            print(f"‚ùå Error al leer {archivo_texto}: {e}")
            return []

        # Limpiar el texto
        contenido = self.limpiar_texto(contenido)

        # Segmentar en oraciones
        oraciones_candidatas = self.segmentar_oraciones(contenido)

        # Filtrar por idioma
        oraciones_awajun = []
        for oracion in oraciones_candidatas:
            if not self.es_probablemente_espanol(oracion):
                oraciones_awajun.append(oracion)

        return oraciones_awajun

    def generar_corpus_monolingue(self, carpeta_origen: str, carpeta_corpus: str) -> None:
        """
        Genera archivos individuales y un corpus consolidado con oraciones awaj√∫n.
        """
        carpeta_origen_path = Path(carpeta_origen)
        carpeta_corpus_path = Path(carpeta_corpus)

        if not carpeta_origen_path.exists():
            print(f"‚ùå La carpeta {carpeta_origen} no existe")
            return

        # Crear carpeta corpus
        carpeta_corpus_path.mkdir(parents=True, exist_ok=True)

        # Buscar archivos de texto procesados
        archivos_texto = list(carpeta_origen_path.glob("*.md")) + list(carpeta_origen_path.glob("*.txt"))

        if not archivos_texto:
            print(f"‚ö†Ô∏è No se encontraron archivos de texto en {carpeta_origen}")
            return

        todas_oraciones = []
        estadisticas_por_archivo = {}

        print(f"\nüîÑ Procesando archivos para extraer awaj√∫n...")

        for archivo in archivos_texto:
            print(f"üìñ Procesando: {archivo.name}")
            oraciones = self.extraer_oraciones_awajun(str(archivo))

            if oraciones:
                # Guardar archivo individual
                nombre_awajun = archivo.stem + "_awajun.txt"
                ruta_awajun = carpeta_corpus_path / nombre_awajun

                with open(ruta_awajun, 'w', encoding='utf-8') as f:
                    for oracion in oraciones:
                        f.write(oracion + '\n')

                todas_oraciones.extend(oraciones)
                estadisticas_por_archivo[archivo.name] = len(oraciones)
                print(f"   ‚úÖ {len(oraciones)} oraciones extra√≠das ‚Üí {nombre_awajun}")
            else:
                estadisticas_por_archivo[archivo.name] = 0
                print(f"   ‚ö†Ô∏è No se encontraron oraciones awaj√∫n v√°lidas")

        # Remover duplicados manteniendo el orden
        oraciones_unicas = list(dict.fromkeys(todas_oraciones))

        # Guardar corpus consolidado
        corpus_consolidado = carpeta_corpus_path / "awajun_corpus_completo.txt"
        with open(corpus_consolidado, 'w', encoding='utf-8') as f:
            for oracion in oraciones_unicas:
                f.write(oracion + '\n')

        # Mostrar estad√≠sticas
        print(f"\nüìä ESTAD√çSTICAS FINALES:")
        print(f"{'=' * 50}")
        print(f"Archivos procesados: {len(archivos_texto)}")
        print(f"\nOraciones por archivo:")
        for archivo, count in estadisticas_por_archivo.items():
            print(f"  ‚Ä¢ {archivo}: {count} oraciones")
        print(f"\nTotales:")
        print(f"  ‚Ä¢ Oraciones extra√≠das: {len(todas_oraciones)}")
        print(f"  ‚Ä¢ Oraciones √∫nicas: {len(oraciones_unicas)}")
        print(f"  ‚Ä¢ Corpus consolidado: {corpus_consolidado}")
        print(f"  ‚Ä¢ Archivos individuales: {carpeta_corpus_path}")
        print(f"{'=' * 50}")


def main():
    """Funci√≥n principal."""
    # Configuraci√≥n
    carpeta_pdfs = "./data"
    carpeta_output = "./output"
    carpeta_corpus = "./corpus"

    # 1. Extraer texto de PDFs a carpeta output
    print("üîÑ Extrayendo texto de PDFs a carpeta output...")
    #extractor = PDFExtractor()
    #extractor.procesar_pdfs(carpeta_pdfs, carpeta_output)

    # 2. Generar corpus monoling√ºe awaj√∫n
    print("\nüîÑ Generando corpus monoling√ºe awaj√∫n...")
    generador = CorpusGenerator()
    generador.generar_corpus_monolingue(carpeta_output, carpeta_corpus)

    print(f"\nüéâ ¬°Proceso completado! Revisa la carpeta {carpeta_corpus}")

if __name__ == '__main__':
    main()